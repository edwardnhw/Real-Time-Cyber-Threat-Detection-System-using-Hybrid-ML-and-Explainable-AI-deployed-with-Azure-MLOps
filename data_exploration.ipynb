{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Problem & Objectives\n",
        "\n",
        "## Business Problem Definition\n",
        "\n",
        "Organization face continuous threats, ranging from external intrusions (e.g., brute-force login attempts, malicious IPs) to insider misuse (e.g., unusual session patterns). Traditional rule-based systems generate high false positives. You aim to build a system that intelligently detects threats by learning from historical behavior, adapting over time, and providing interpretable results.\n",
        "\n",
        "### Use Case:\n",
        "Build a cyber threat detection system that combines: \n",
        "- **Supervised Classification** for known threats  \n",
        "- **Explainability** to support compliance and human trust  \n",
        "\n",
        "### Key Objectives:\n",
        "- Automatically flag suspicious sessions or network activities  \n",
        "- Support SOC analysts by providing a reason or risk score for each alert  \n",
        "- Automate retraining and deployment using Azure MLOps tools  \n",
        "- Log predictions and monitor for drift or changes in behavior patterns  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Storage Service Reevaluation Based on Dataset Size and Use Case\n",
        "\n",
        "**Source:** [Kaggle - Cybersecurity Intrusion Detection Dataset](https://www.kaggle.com/datasets/dnkumars/cybersecurity-intrusion-detection-dataset/data)\n",
        "\n",
        "## Dataset Profile\n",
        "- **Rows:** ~9,000  \n",
        "- **Format:** Structured tabular (CSV)  \n",
        "- **Size:** Likely under 10 MB  \n",
        "- **Usage:** For ML model development, training, versioning  \n",
        "- **Access pattern:** Infrequent writes, multiple reads (especially during training)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the relatively small size and simplicity of access required for this project, Azure Blob Storage was selected as the primary storage solution. It provides just the right balance of flexibility, cost-efficiency, and integration with the Azure ML ecosystem.\n",
        "\n",
        "## Benefits:\n",
        "- Simple to configure and manage  \n",
        "- Fully supported by Azure Machine Learning, Azure Databricks, and Azure Data Factory  \n",
        "- Seamlessly integrates with versioned datasets in Azure ML  \n",
        "- Cost-effective choice for small, structured datasets  \n",
        "- Supports secure access through:\n",
        "  - Azure Active Directory with role-based access control (RBAC)  \n",
        "  - Private Endpoints for secure network isolation  \n",
        "  - Storage-level encryption  \n",
        "- Easily scalable if data volume increases in the future  \n",
        "\n",
        "## Implementation:\n",
        "- Created a new storage account within the Azure environment  \n",
        "- Enabled Blob Storage (GPv2) as the storage tier  \n",
        "- Enforced HTTPS-only communication for secure data transfer  \n",
        "- Enabled Soft Delete to provide protection against accidental data removal  \n",
        "- Provisioned a dedicated container (e.g., `ml-data-intrusion`) for the ML assets  \n",
        "- Uploaded the dataset (`intrusion_dataset.csv`) to the storage container  \n",
        "\n",
        "## Secure Access:\n",
        "- Assigned the appropriate RBAC roles (Storage Blob Data Reader or Contributor) to the Azure ML and Databricks service identities  \n",
        "- Configured Private Endpoint access to ensure the storage account is not publicly accessible in production  \n",
        "\n",
        "## Integration with Azure ML:\n",
        "- Registered the dataset in Azure ML using `AssetTypes.URI_FILE`  \n",
        "- Provided the direct path to the CSV file hosted in Blob Storage for subsequent use in training and pipeline workflows  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the Dataset in Azure Machine Learning\n",
        "## Step 1: Create a Container in Azure Blob Storage\n",
        "\n",
        "To begin organizing the data assets, I created a dedicated container within the Blob Storage account.\n",
        "\n",
        "In the upload window:\n",
        "- Clicked **\"Create new\"** under the *Select an existing container* dropdown  \n",
        "- Named the container using all lowercase characters with no special characters or spaces — for this project, I used:  \n",
        "  - `intrusiondata`  \n",
        "- Confirmed creation by clicking **Create**\n",
        "\n",
        "## Step 2: Upload the Dataset\n",
        "\n",
        "With the `intrusiondata` container selected:\n",
        "- Clicked **Upload** to add the dataset  \n",
        "- Uploaded the `cybersecurity_intrusion_data.csv` file  \n",
        "\n",
        "The file is now available at the following path:\n",
        "\n",
        "https://mldataintrusion.blob.core.windows.net/intrusiondata/cybersecurity_intrusion_data.csv\n",
        "\n",
        "\n",
        "This path is important—I'll be using it to reference the dataset inside Azure Machine Learning.\n",
        "\n",
        "## Step 3: Register the Dataset in Azure Machine Learning\n",
        "\n",
        "Now that the dataset is securely stored in Blob Storage, I registered it in Azure Machine Learning. This enables version control, traceability, and easy reuse in training workflows and pipelines.\n",
        "\n",
        "### Register via Azure ML Studio\n",
        "\n",
        "1. Opened [Azure Machine Learning Studio](https://ml.azure.com)  \n",
        "2. Navigated to the target ML Workspace  \n",
        "3. In the left-hand menu, selected **Data → + Create**  \n",
        "4. Filled out the dataset details:\n",
        "   - **Type:** Tabular  \n",
        "   - **Name:** `cyber_intrusion_dataset`  \n",
        "   - **Description:** (left optional)  \n",
        "   - **Datastore:** Selected the default linked to my storage account  \n",
        "   - **Path:** Browsed to the uploaded CSV in the `intrusiondata` container  \n",
        "5. Walked through the wizard and completed the dataset registration\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an Azure Machine Learning Workspace\n",
        "\n",
        "To manage experiments, datasets, models, environments, and compute resources in one place, I created an Azure ML workspace.\n",
        "\n",
        "This workspace acts as the central hub for all machine learning operations in the project.\n",
        "\n",
        "### Step 1: How to Create an Azure ML Workspace\n",
        "\n",
        "#### Method: Azure Portal (GUI)\n",
        "\n",
        "1. Navigated to [Azure Portal](https://portal.azure.com)  \n",
        "2. Clicked **Create a resource** → searched for **Machine Learning**  \n",
        "3. Clicked **Create** and filled out the workspace configuration form:\n",
        "\n",
        "| Field            | Value                                      |\n",
        "|------------------|--------------------------------------------|\n",
        "| **Subscription** | Azure for Students                         |\n",
        "| **Resource Group** | `cyberml-canada-rg` (already created)     |\n",
        "| **Workspace name** | `cyberml-ws` (or similar, lowercase)      |\n",
        "| **Region**       | Canada Central (to match the storage region) |\n",
        "| **Storage account** | Selected `mldataintrusion`               |\n",
        "| **Key vault**    | Auto-created or selected existing          |\n",
        "| **App Insights** | Auto-created                               |\n",
        "| **Container Registry** | Optional (created if prompted)       |\n",
        "\n",
        "4. Clicked **Review + Create**, validated the configuration, and deployed the workspace  \n",
        "\n",
        "Once deployment completed:\n",
        "- Navigated to [Azure ML Studio](https://ml.azure.com)  \n",
        "- Switched to the new workspace using the dropdown in the top menu\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡️ Alternative: Azure CLI (Faster)\n",
        "\n",
        "If using the CLI, the workspace can be created with the following command:\n",
        "\n",
        "```bash\n",
        "az ml workspace create \\\n",
        "  --name cyberml-ws \\\n",
        "  --resource-group cyberml-canada-rg \\\n",
        "  --location canadacentral\n",
        "  ```\n",
        "---\n",
        "This will provision the workspace with default settings and link it to the specified resource group and region.\n",
        "\n",
        "With the workspace in place, I’m now able to:\n",
        "\n",
        "- Register datasets\n",
        "- Run and manage notebooks, experiments, and pipelines\n",
        "- Train and register models\n",
        "- Monitor and manage model deployments\n",
        "\n",
        "---\n",
        "## ✅ Once the Workspace Is Created\n",
        "\n",
        "With the Azure ML workspace successfully deployed, I returned to [Azure Machine Learning Studio](https://ml.azure.com) to begin working within it.\n",
        "\n",
        "### Switching to the New Workspace\n",
        "- Used the **top-left workspace dropdown**  \n",
        "- Selected my workspace: `cyberml-ws`\n",
        "\n",
        "### Verifying Access to Data Management\n",
        "Once inside the correct workspace:\n",
        "- Navigated to the **Data** section from the left-hand menu  \n",
        "- Clicked **+ Create**\n",
        "\n",
        "At this point, the **Create** option was enabled, confirming that the workspace was fully active and ready for dataset registration and ML pipeline development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration & Preprocessing\n",
        "\n",
        "With the dataset now registered in Azure ML in the workspace, I proceeded to explore and preprocess the data in preparation for modeling.\n",
        "\n",
        "### 🔍 What I Covered in This Step:\n",
        "- Loaded the dataset in a notebook environment within Azure ML Studio  \n",
        "- Explored the schema and overall data quality  \n",
        "- Checked for missing values and data type inconsistencies  \n",
        "\n",
        "### 🧹 Data Cleaning & Transformation:\n",
        "- Converted categorical variables to numerical formats (e.g., label encoding or one-hot)  \n",
        "- Handled any null values or inconsistent data types  \n",
        "- Scaled or normalized numerical columns as needed  \n",
        "- Optionally: Saved a cleaned version of the dataset back to Blob Storage or registered it as a new dataset version in Azure ML  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Launching a Notebook in Azure ML Studio\n",
        "\n",
        "#### 1. Create the Notebook\n",
        "- Navigated to **Notebooks** from the left-hand menu in [Azure ML Studio](https://ml.azure.com)  \n",
        "- Clicked **User files**  \n",
        "- Selected **+ New file → Notebook**  \n",
        "- Named the notebook:  \n",
        "  - `01_data_exploration.ipynb`\n",
        "\n",
        "#### 2. Attach Compute (If Not Yet Created)\n",
        "- Clicked **Select compute** in the top-right corner of the notebook interface  \n",
        "- If no compute was available, I created a new one with the following settings:\n",
        "\n",
        "| Field       | Value                                |\n",
        "|-------------|----------------------------------------|\n",
        "| **Name**    | `cpu-cluster`                         |\n",
        "| **VM Size** | `Standard_DS11_v2` (or any free-tier eligible) |\n",
        "| **Min nodes** | 0                                    |\n",
        "| **Max nodes** | 1                                    |\n",
        "\n",
        "- Waited for the compute instance to start before running any cells\n",
        "\n",
        "Once everything was set up, I began exploring the dataset and preparing it for the next phase: feature engineering and model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering and Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Connect to Azure ML Workspace\n",
        "\n",
        "We connect to our Azure ML Workspace using `MLClient` and `DefaultAzureCredential`. This allows us to access registered assets like datasets and models from our workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install azure-ai-ml\n",
        "%pip install azure-identity\n",
        "from azure.ai.ml import MLClient, Input, load_component\n",
        "from azure.ai.ml.entities import BatchEndpoint, ModelBatchDeployment, ModelBatchDeploymentSettings, PipelineComponentBatchDeployment, Model, AmlCompute, Data, BatchRetrySettings, CodeConfiguration, Environment, Data\n",
        "from azure.ai.ml.constants import AssetTypes, BatchDeploymentOutputAction\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.identity import DefaultAzureCredential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# First, create MLTable file with correct structure\n",
        "os.makedirs(\"mltable_cyber_intrusion\", exist_ok=True)\n",
        "\n",
        "mltable_content = \"\"\"type: mltable\n",
        "paths:\n",
        "  - file: ../cybersecurity_intrusion_data.csv\n",
        "transformations:\n",
        "  - read_delimited:\n",
        "      delimiter: ','\n",
        "      encoding: utf8\n",
        "      header: all_files_same_headers\n",
        "\"\"\"\n",
        "\n",
        "# Write MLTable file\n",
        "with open(\"mltable_cyber_intrusion/MLTable\", \"w\") as f:\n",
        "    f.write(mltable_content)\n",
        "\n",
        "# Connect to Azure ML Workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "try:\n",
        "    # Register MLTable dataset using absolute path\n",
        "    mltable_data = Data(\n",
        "        path=os.path.abspath(\"mltable_cyber_intrusion\"),\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        name=\"cyber_intrusion_dataset_mltable\",\n",
        "        description=\"Cyber intrusion dataset in MLTable format\",\n",
        "        version=\"1\"\n",
        "    )\n",
        "    \n",
        "    result = ml_client.data.create_or_update(mltable_data)\n",
        "    print(f\"Dataset registered successfully. Name: {result.name}, Version: {result.version}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error registering dataset: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Load and Explore the Dataset\n",
        "We retrieve the registered `cyber_intrusion_dataset` from Azure ML and load it into a pandas DataFrame. We examine the dataset shape, columns, and view a sample of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1743370222918
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Get data asset (uri_file type)\n",
        "data_asset = ml_client.data.get(name=\"cyber_intrusion_dataset\", version=\"1\")\n",
        "\n",
        "# Download the actual file locally from the URI\n",
        "download_path = data_asset.path\n",
        "\n",
        "# Load the CSV directly from its storage URI\n",
        "df = pd.read_csv(download_path)\n",
        "\n",
        "# Explore\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Data Cleaning and Preprocessing\n",
        "\n",
        "### ✔ 3.1 Check for Issues\n",
        "We check for missing values, duplicated rows, and inspect data types to confirm data consistency.\n",
        "\n",
        "### ✔ 3.2 Summary Statistics\n",
        "We use `describe()` to understand the distribution of numeric features, helping inform transformations.\n",
        "\n",
        "### ✔ 3.3 Encode Categorical Features\n",
        "Categorical columns `protocol_type`, `encryption_used`, and `browser_type` are one-hot encoded to make them machine learning-ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "# Check column data types\n",
        "print(\"\\nColumn data types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# Statistical overview for numeric columns\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# List of categorical columns to encode\n",
        "categorical_cols = ['protocol_type', 'encryption_used', 'browser_type']\n",
        "\n",
        "# Apply one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Show updated columns\n",
        "print(\"New columns after encoding:\")\n",
        "print(df_encoded.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Analyze Class Distribution\n",
        "We check the balance of our target variable `attack_detected` to understand class imbalance. This will guide how we evaluate model performance (e.g., accuracy vs. F1-score).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# Check target class distribution\n",
        "df_encoded['attack_detected'].value_counts(normalize=True) * 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Normalize Numerical Features\n",
        "We apply Min-Max scaling to numeric features to ensure equal weighting during model training. This is especially helpful for distance-based or gradient-based algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select numeric features to scale (excluding label)\n",
        "num_features = ['network_packet_size', 'login_attempts', 'session_duration', 'ip_reputation_score', 'failed_logins']\n",
        "\n",
        "# Initialize scaler and apply\n",
        "scaler = MinMaxScaler()\n",
        "df_encoded[num_features] = scaler.fit_transform(df_encoded[num_features])\n",
        "\n",
        "# Confirm scaling\n",
        "df_encoded[num_features].describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Train-Test Split & Baseline Model Training\n",
        "We split the preprocessed data into training and testing sets, then train a baseline Logistic Regression model to establish initial performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Separate features and target\n",
        "X = df_encoded.drop(['session_id', 'attack_detected'], axis=1)\n",
        "y = df_encoded['attack_detected']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train baseline Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 7: Train Using Azure AutoML (SDK)\n",
        "\n",
        "In this step, we use Azure AutoML to automate model selection, hyperparameter tuning, and training. This approach helps identify the best model without manual trial-and-error and ensures reproducibility.\n",
        "\n",
        "We specify the compute target, data source, task type (classification), target column, and primary evaluation metric (F1-score).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "ml_client.data.get(name=\"cyber_intrusion_dataset_mltable\", version=\"1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "import os\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# 1. Setup paths\n",
        "base_dir = \"mltable_cyber_intrusion\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# 2. Rewrite MLTable definition correctly\n",
        "mltable_yaml = \"\"\"paths:\n",
        "  - file: ./cybersecurity_intrusion_data.csv\n",
        "\n",
        "transformations:\n",
        "  - read_delimited:\n",
        "      delimiter: \",\"\n",
        "      encoding: utf8\n",
        "      header: all_files_same_headers\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(base_dir, \"MLTable\"), \"w\") as f:\n",
        "    f.write(mltable_yaml)\n",
        "\n",
        "# 3. Register the MLTable dataset with absolute path\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "mltable_data = Data(\n",
        "    path=os.path.abspath(base_dir),\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    name=\"cyber_intrusion_dataset_mltable\",\n",
        "    description=\"Cyber intrusion dataset in MLTable format\",\n",
        "    version=\"3\",  # Use a new version!\n",
        ")\n",
        "\n",
        "result = ml_client.data.create_or_update(mltable_data)\n",
        "print(f\"Dataset registered: {result.name}, Version: {result.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.automl import classification\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import time\n",
        "\n",
        "# Connect to ML workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Load training data (MLTable)\n",
        "my_training_data = Input(\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    path=\"azureml:cyber_intrusion_dataset_mltable:2\"\n",
        ")\n",
        "\n",
        "# Define AutoML classification job\n",
        "automl_classification_job = classification(\n",
        "    training_data=my_training_data,\n",
        "    target_column_name=\"attack_detected\",\n",
        "    compute=\"cpucluster01\",\n",
        "    experiment_name=\"cyber_intrusion_detection_automl\",\n",
        "    primary_metric=\"accuracy\",\n",
        "    n_cross_validations=5,\n",
        "    enable_model_explainability=True,\n",
        "    tags={\"training_type\": \"automl\", \"attack_detection\": \"cybersecurity\"}\n",
        ")\n",
        "\n",
        "# Optional: Training config\n",
        "automl_classification_job.set_training(\n",
        "    enable_stack_ensemble=True,\n",
        "    enable_vote_ensemble=True\n",
        ")\n",
        "\n",
        "# Limits\n",
        "automl_classification_job.set_limits(\n",
        "    max_trials=20,\n",
        "    max_concurrent_trials=4,\n",
        "    timeout_minutes=180,\n",
        "    enable_early_termination=True\n",
        ")\n",
        "\n",
        "# Submit the job\n",
        "returned_job = ml_client.jobs.create_or_update(automl_classification_job)\n",
        "\n",
        "# Print job info\n",
        "print(f\"Submitted AutoML job: {returned_job.name}\")\n",
        "print(f\"Monitor in Azure ML Studio: {returned_job.studio_url}\")\n",
        "\n",
        "# Optional: Live monitoring\n",
        "while True:\n",
        "    job = ml_client.jobs.get(returned_job.name)\n",
        "    print(f\"Job status: {job.status}\")\n",
        "    if job.status in ['Completed', 'Failed', 'Canceled']:\n",
        "        break\n",
        "    time.sleep(60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Register the Best Model\n",
        "\n",
        "Following the completion of the AutoML experiment, the best-performing model from the run was registered in Azure Machine Learning. This step ensures the model is versioned, reproducible, and ready for deployment or integration into downstream pipelines.\n",
        "\n",
        "> **Note:** The job name used for registration was taken from a previously successful AutoML run. The job ID was retrieved from the Azure ML Studio under the Jobs section.\n",
        "\n",
        "By registering the model:\n",
        "- The asset becomes accessible in the Azure ML model registry  \n",
        "- Model versioning and lifecycle management are enabled  \n",
        "- Future steps such as deployment, batch scoring, or monitoring can reference the registered version directly  \n",
        "- CI/CD and MLOps pipelines can leverage the model consistently across environments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# Step 8: Register the Best AutoML Model\n",
        "\n",
        "# Import necessary libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azureml.core import Workspace\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Connect to Azure ML Workspace\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve the AutoML parent run name from earlier step\n",
        "parent_run_id = \"amusing_skin_nw68t1k275\" \n",
        "\n",
        "# Connect to MLflow tracking URI\n",
        "ws = Workspace.from_config()\n",
        "mlflow_client = MlflowClient()\n",
        "\n",
        "# Get the parent AutoML run\n",
        "parent_run = mlflow_client.get_run(parent_run_id)\n",
        "\n",
        "# Extract best child run ID\n",
        "best_child_run_id = parent_run.data.tags.get(\"automl_best_child_run_id\")\n",
        "if not best_child_run_id:\n",
        "    raise ValueError(\"Best child run ID not found. Ensure AutoML run completed successfully.\")\n",
        "\n",
        "print(f\"Best child run ID: {best_child_run_id}\")\n",
        "\n",
        "# Define model path in outputs\n",
        "model_path = f\"azureml://jobs/{best_child_run_id}/outputs/artifacts/outputs/mlflow-model/\"\n",
        "\n",
        "# Create and register the model\n",
        "model = Model(\n",
        "    path=model_path,\n",
        "    name=\"cyber_intrusion_model\",\n",
        "    description=\"Best model from AutoML run for cybersecurity intrusion detection\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)\n",
        "\n",
        "print(f\"Model registered: {registered_model.name}, version: {registered_model.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 9: Review and Track Model Performance\n",
        "\n",
        "Before deploying the model, we review its performance, including metrics like precision, recall, and accuracy. We also explore explainability insights to validate model behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from mlflow.tracking import MlflowClient\n",
        "from azureml.core import Workspace\n",
        "import mlflow\n",
        "\n",
        "# Connect to ML Workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "ws = Workspace.from_config()\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow_client = MlflowClient()\n",
        "\n",
        "# Define parent and best child run\n",
        "parent_run_id = \"amusing_skin_nw68t1k275\"\n",
        "parent_run = mlflow_client.get_run(parent_run_id)\n",
        "best_child_run_id = parent_run.data.tags[\"automl_best_child_run_id\"]\n",
        "\n",
        "# Load best run metrics\n",
        "best_run = mlflow_client.get_run(best_child_run_id)\n",
        "metrics = best_run.data.metrics\n",
        "print(\"Best model metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# Open in Azure ML Studio (AutoML Run link)\n",
        "print(f\"\\n View full experiment in Azure ML Studio:\")\n",
        "print(f\"https://ml.azure.com/runs/{parent_run_id}?wsid=/subscriptions/{ml_client.subscription_id}/resourceGroups/{ml_client.resource_group_name}/workspaces/{ml_client.workspace_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps: MLflow Batch Inference (Continued)\n",
        "\n",
        "To streamline the batch inference process and align with Azure Machine Learning best practices, the work continues in a separate notebook that adopts the MLflow format for batch deployments.\n",
        "\n",
        "### Transition Overview\n",
        "\n",
        "The following artifacts have been prepared and structured to support this transition:\n",
        "\n",
        "- `mlflow-for-batch-tabular.ipynb`  \n",
        "  Contains the implementation for batch inference using MLflow-based deployment and scoring.\n",
        "\n",
        "- `environment/`  \n",
        "  Defines the environment configuration using a `conda` environment file (e.g., `env.yml`) including MLflow, scikit-learn, and Azure ML SDK dependencies.\n",
        "\n",
        "- `code/`  \n",
        "  Contains the `score.py` file structured with `init()` and `run()` functions for use in MLflow batch deployments.\n",
        "\n",
        "- `named-outputs/score/`  \n",
        "  Folder designated for storing batch inference outputs, such as the generated `predictions.csv`.\n",
        "\n",
        "- `cybersecurity_intrusion_data_test.csv`  \n",
        "  Sample dataset used to test batch inference jobs.\n",
        "\n",
        "### Next Notebook\n",
        "\n",
        "Please proceed to the notebook:\n",
        "\n",
        "`mlflow-for-batch-tabular.ipynb`\n",
        "\n",
        "This notebook walks through:\n",
        "- Registering or referencing an MLflow model\n",
        "- Creating a batch endpoint and deployment\n",
        "- Submitting a batch inference job\n",
        "- Verifying and reviewing output predictions\n",
        "\n",
        "By structuring the process this way, we ensure separation between exploratory analysis and production inference workflows, promoting modularity and scalability in the MLOps lifecycle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attempt to Deploy a Real-Time Endpoint (Optional Exploration)\n",
        "\n",
        "As part of exploring different deployment strategies, an attempt was made to deploy the model to an **Azure ML managed online endpoint** to evaluate real-time inference capabilities.\n",
        "\n",
        "### Rationale for Real-Time Endpoint Exploration\n",
        "While the core use case and resource constraints justified batch inference, deploying to a real-time endpoint offered a chance to:\n",
        "- Experiment with REST-based, low-latency predictions  \n",
        "- Simulate production-grade API integration scenarios  \n",
        "- Benchmark deployment and containerization processes on Azure\n",
        "\n",
        "### Outcome & Limitation\n",
        "The deployment was initially successful in terms of endpoint provisioning (`cyber-intrusion-endpoint-v1`), but failed during the container startup phase with the following error:\n",
        "\n",
        "```\n",
        "(ResourceNotReady) User container has crashed or terminated.\n",
        "Minimum recommended compute SKU is Standard_DS3_v2 for general-purpose online endpoints.\n",
        "```\n",
        "\n",
        "\n",
        "The current workspace was using `Standard_DS2_v2`, which is part of the **student subscription** and may not meet the baseline compute requirements for real-time scoring containers. As a result, the scoring container failed to initialize and crashed during deployment.\n",
        "\n",
        "### Cleanup\n",
        "Post-failure, the endpoint was programmatically cleaned up to avoid incurring unused resources.\n",
        "\n",
        "---\n",
        "\n",
        "### Note for Future Iterations\n",
        "Real-time endpoints are highly recommended in production settings where low-latency, event-driven predictions are required—especially for cybersecurity systems that need to trigger immediate responses.\n",
        "\n",
        "For users with access to **higher-tier Azure subscriptions**, it's encouraged to:\n",
        "- Use `Standard_DS3_v2` or higher SKUs  \n",
        "- Explore **managed online endpoints** with autoscaling and authentication features  \n",
        "- Benchmark response latency vs. cost trade-offs\n",
        "\n",
        "---\n",
        "\n",
        "Although this part of the project did not succeed under current constraints, it provided valuable insights into environment compatibility and production-readiness considerations in Azure ML.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found model: cyber_intrusion_model version 11\n",
            "No existing endpoint found with name: cyberintrusionendpointv1\n",
            "Creating endpoint...\n",
            "Endpoint cyberintrusionendpointv1 created successfully\n",
            "Creating deployment...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint cyberintrusionendpointv1 exists\n",
            "Uploading code (21.44 MBs): 100%|██████████| 21440276/21440276 [00:00<00:00, 21612867.54it/s]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Error: (BadRequest) The request is invalid.\n",
            "Code: BadRequest\n",
            "Message: The request is invalid.\n",
            "Exception Details:\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\"}\"}}\n",
            "\tCode: InferencingClientCallFailed\n",
            "\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\"errors\\\":{\\\"VmSize\\\":[\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\"]},\\\"type\\\":\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\",\\\"title\\\":\\\"One or more validation errors occurred.\\\",\\\"status\\\":400,\\\"traceId\\\":\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\"}\"}}\n",
            "Additional Information:Type: ComponentName\n",
            "Info: {\n",
            "    \"value\": \"managementfrontend\"\n",
            "}Type: Correlation\n",
            "Info: {\n",
            "    \"value\": {\n",
            "        \"operation\": \"f2a5699fa6470ff757e34bab0327809c\",\n",
            "        \"request\": \"74dc442840ddab0d\"\n",
            "    }\n",
            "}Type: Environment\n",
            "Info: {\n",
            "    \"value\": \"canadacentral\"\n",
            "}Type: Location\n",
            "Info: {\n",
            "    \"value\": \"canadacentral\"\n",
            "}Type: Time\n",
            "Info: {\n",
            "    \"value\": \"2025-04-05T00:40:28.6892621+00:00\"\n",
            "}\n",
            "Error type: HttpResponseError\n",
            "Full error details: {'reason': 'The request is invalid.', 'status_code': 400, 'response': <RequestsTransportResponse: 400 The request is invalid., Content-Type: application/json; charset=utf-8>, 'model': None, 'error': <azure.mgmt.core.exceptions.ARMErrorFormat object at 0x7fccf81f2e90>, 'inner_exception': None, 'exc_type': <class 'NoneType'>, 'exc_value': None, 'exc_traceback': None, 'exc_msg': '(BadRequest) The request is invalid.\\nCode: BadRequest\\nMessage: The request is invalid.\\nException Details:\\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\\\"errors\\\\\":{\\\\\"VmSize\\\\\":[\\\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\\\"]},\\\\\"type\\\\\":\\\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\\\",\\\\\"title\\\\\":\\\\\"One or more validation errors occurred.\\\\\",\\\\\"status\\\\\":400,\\\\\"traceId\\\\\":\\\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\\\"}\"}}\\n\\tCode: InferencingClientCallFailed\\n\\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\\\"errors\\\\\":{\\\\\"VmSize\\\\\":[\\\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\\\"]},\\\\\"type\\\\\":\\\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\\\",\\\\\"title\\\\\":\\\\\"One or more validation errors occurred.\\\\\",\\\\\"status\\\\\":400,\\\\\"traceId\\\\\":\\\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\\\"}\"}}\\nAdditional Information:Type: ComponentName\\nInfo: {\\n    \"value\": \"managementfrontend\"\\n}Type: Correlation\\nInfo: {\\n    \"value\": {\\n        \"operation\": \"f2a5699fa6470ff757e34bab0327809c\",\\n        \"request\": \"74dc442840ddab0d\"\\n    }\\n}Type: Environment\\nInfo: {\\n    \"value\": \"canadacentral\"\\n}Type: Location\\nInfo: {\\n    \"value\": \"canadacentral\"\\n}Type: Time\\nInfo: {\\n    \"value\": \"2025-04-05T00:40:28.6892621+00:00\"\\n}, NoneType: None', 'message': '(BadRequest) The request is invalid.\\nCode: BadRequest\\nMessage: The request is invalid.\\nException Details:\\t(InferencingClientCallFailed) {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\\\"errors\\\\\":{\\\\\"VmSize\\\\\":[\\\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\\\"]},\\\\\"type\\\\\":\\\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\\\",\\\\\"title\\\\\":\\\\\"One or more validation errors occurred.\\\\\",\\\\\"status\\\\\":400,\\\\\"traceId\\\\\":\\\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\\\"}\"}}\\n\\tCode: InferencingClientCallFailed\\n\\tMessage: {\"error\":{\"code\":\"Validation\",\"message\":\"{\\\\\"errors\\\\\":{\\\\\"VmSize\\\\\":[\\\\\"Not enough quota available for Standard_DS3_v2 in SubscriptionId 43dae9af-3755-421b-bfae-29b91f9e85dd. Current usage/limit: 2/6. Additional needed: 8 Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota\\\\\"]},\\\\\"type\\\\\":\\\\\"https://tools.ietf.org/html/rfc9110#section-15.5.1\\\\\",\\\\\"title\\\\\":\\\\\"One or more validation errors occurred.\\\\\",\\\\\"status\\\\\":400,\\\\\"traceId\\\\\":\\\\\"00-f2a5699fa6470ff757e34bab0327809c-98a4b4a33421a756-01\\\\\"}\"}}\\nAdditional Information:Type: ComponentName\\nInfo: {\\n    \"value\": \"managementfrontend\"\\n}Type: Correlation\\nInfo: {\\n    \"value\": {\\n        \"operation\": \"f2a5699fa6470ff757e34bab0327809c\",\\n        \"request\": \"74dc442840ddab0d\"\\n    }\\n}Type: Environment\\nInfo: {\\n    \"value\": \"canadacentral\"\\n}Type: Location\\nInfo: {\\n    \"value\": \"canadacentral\"\\n}Type: Time\\nInfo: {\\n    \"value\": \"2025-04-05T00:40:28.6892621+00:00\"\\n}', 'continuation_token': None}\n",
            "\n",
            "Attempting to clean up endpoint cyberintrusionendpointv1\n",
            "...Cleanup completed\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Environment\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "from azure.ai.ml.entities import CodeConfiguration\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize the MLClient\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Configuration\n",
        "endpoint_name = \"cyberintrusionendpointv1\"\n",
        "deployment_name = \"blue\"\n",
        "model_name = \"cyber_intrusion_model\"\n",
        "model_version = \"11\"\n",
        "\n",
        "# Use curated AzureML environment to avoid image build issues\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env = ml_client.environments.get(\n",
        "    name=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\",\n",
        "    version=\"1\"\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    # Verify model exists\n",
        "    model = ml_client.models.get(name=model_name, version=model_version)\n",
        "    print(f\"Found model: {model.name} version {model.version}\")\n",
        "\n",
        "    # Delete existing endpoint if present\n",
        "    try:\n",
        "        existing = ml_client.online_endpoints.get(name=endpoint_name)\n",
        "        print(f\"Deleting existing endpoint: {endpoint_name}\")\n",
        "        ml_client.online_endpoints.begin_delete(name=endpoint_name).result()\n",
        "    except ResourceNotFoundError:\n",
        "        print(f\"No existing endpoint found with name: {endpoint_name}\")\n",
        "\n",
        "    # Create endpoint\n",
        "    print(\"Creating endpoint...\")\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=endpoint_name,\n",
        "        description=\"Real-time endpoint for cybersecurity intrusion detection\",\n",
        "        auth_mode=\"key\",\n",
        "        tags={\n",
        "            \"project\": \"cyber_intrusion\",\n",
        "            \"type\": \"realtime\",\n",
        "            \"model_name\": model_name,\n",
        "            \"model_version\": model_version\n",
        "        }\n",
        "    )\n",
        "    ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "    print(f\"Endpoint {endpoint_name} created successfully\")\n",
        "\n",
        "    # Create deployment with curated environment\n",
        "    print(\"Creating deployment...\")\n",
        "    deployment = ManagedOnlineDeployment(\n",
        "        name=deployment_name,\n",
        "        endpoint_name=endpoint_name,\n",
        "        model=model,\n",
        "        environment=env,\n",
        "        code_configuration=CodeConfiguration(code=\"./\", scoring_script=\"score.py\"),\n",
        "        instance_type=\"Standard_DS3_v2\",\n",
        "        instance_count=1\n",
        "    )\n",
        "\n",
        "    ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
        "    print(f\"Deployment {deployment_name} created successfully\")\n",
        "\n",
        "    # Update traffic\n",
        "    endpoint.traffic = {deployment_name: 100}\n",
        "    ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "    print(\"Traffic allocation updated\")\n",
        "\n",
        "    # Output endpoint info\n",
        "    endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
        "    print(\"\\nEndpoint Details:\")\n",
        "    print(f\"Name: {endpoint.name}\")\n",
        "    print(f\"State: {endpoint.provisioning_state}\")\n",
        "    print(f\"URI: {endpoint.scoring_uri}\")\n",
        "\n",
        "    # Retrieve primary key\n",
        "    key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
        "    print(\"\\nAuthentication key retrieved successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: {str(e)}\")\n",
        "    print(f\"Error type: {type(e).__name__}\")\n",
        "    print(f\"Full error details: {e.__dict__}\")\n",
        "    try:\n",
        "        print(f\"\\nAttempting to clean up endpoint {endpoint_name}\")\n",
        "        ml_client.online_endpoints.begin_delete(name=endpoint_name).result()\n",
        "        print(\"Cleanup completed\")\n",
        "    except Exception as cleanup_error:\n",
        "        print(f\"Cleanup error: {str(cleanup_error)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
